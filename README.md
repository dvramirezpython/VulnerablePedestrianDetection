# VulnerablePedestrianDetection
This project focuses on the development of a real-time vulnerable pedestrian detection system capable of operating reliably in both normal and adverse weather conditions. The system integrates state-of-the-art deep learning object detection models, specifically YOLO (You Only Look Once) and RT-DETR (Real-Time DEtection TRansformer), to balance speed and accuracy.

The solution is optimized for deployment on edge devices, ensuring low-latency performance and efficient resource utilization without compromising detection quality. By leveraging model quantization, pruning, and lightweight optimization techniques, the system achieves real-time inference suitable for resource-constrained environments such as embedded GPUs, NPUs, and industrial IoT devices.

Special attention is given to robustness in adverse weather scenarios (e.g., rain, fog, low light), where traditional vision-based methods often fail. The system incorporates weather-augmented datasets and domain-adaptive training strategies to enhance detection accuracy across diverse environmental conditions.

This project demonstrates a scalable, deployable framework that addresses critical challenges in pedestrian safety applications, including autonomous driving, smart surveillance, and assistive technologies, with the goal of minimizing accidents involving vulnerable pedestrians.
